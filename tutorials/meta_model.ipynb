{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dMYrLSWzJs3"
   },
   "source": [
    "# **Introduction to surrogate Modelling**\n",
    "\n",
    "Pierre Kerfriden, Mines ParisTech / Cardiff University\n",
    "\n",
    "## **Content:**\n",
    "\n",
    "- Exercise 1. Frequentist model selection\n",
    "- Exercise 2. Bayesian model selection\n",
    "- Exercise 3. Nonparametric bayesian modelling\n",
    "- Exercise 4. Bayesian optimisation\n",
    "\n",
    "[Link to lecture slides](http://1drv.ms/b/s!AjM6vw3llOZ-iLQsUnA1fDuXluTVQg)\n",
    "\n",
    "[Supplementary material](https://1drv.ms/b/s!AjM6vw3llOZ-i5I0v-aZCC9_Rjfo_w?e=y6TSoi)\n",
    "\n",
    "Run the notebook in Google colab:\n",
    "https://colab.research.google.com/github/heprom/cvml/blob/main/tutorials/meta_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI6FcMHd942b"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Exercise 1. Frequentist model selection**\n",
    "\n",
    "1. Propose an algorithm to select the polynomial order optimally. The number of model evaluations is fixed for now (*e.g.* 10 points) \n",
    "2. Propose an algorithm to select number of model evaluations optimally when the polynomial order is fixed\n",
    "3. Propose an algorithm to select the polynomial order AND number of model evaluations optimally\n",
    "4. Propose an algorithm to select the polynomial order, number of model evaluations and ridge regularisation parameter optimally\n",
    "\n",
    "Note: np.random.seed(x) allows you to fix the seed of the random generator. If unchanged, the code will generate the same pseudo-random number every time it is ran. Change x if you want to generate new random evaluation points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciQQRmoVW0lt"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Exercise 2. Bayesian model selection** \n",
    "\n",
    "- Select the polynomial order for N=30 model evaluation (random sequence)\n",
    "- Suggest an algorithm for choosing the polynomial order (which may differ for the two input dimensions) and the number of model evaluations\n",
    "\n",
    "Note: you may also switch to a compressive sensing model (TypeRegularisation = 1) and investigate the effect of the corresponding regularisation coefficient (RegulCoeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EiEMVwZ6_zw"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Exercise 3: Nonparametric bayesian modelling: Gaussian Process Regression**\n",
    "\n",
    "1. Implement an exponential Kernel: modify the Covariance matrix and the cross-covariance vector\n",
    "2. Implement the automatic choice of the covariance length scale by maximising the data likelihood (at the moment, the amplitude of the Kernel is optimised\n",
    "3. Optimise both the amplitude and length-scale parameters\n",
    "\n",
    "Note: look for comments \"... is to be updated\" in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDAffda3OY_A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "from scipy.optimize import fmin_bfgs, fmin, fminbound\n",
    "\n",
    "class SimpleModel1D:\n",
    "    \n",
    "    def eval(self,x) :\n",
    "        f = np.sin(3.0*x-2.25) + abs(x) + x\n",
    "        return f\n",
    "    \n",
    "    def plot(self,xLim,NPoint):\n",
    "        xPlot = np.linspace(xLim[0],xLim[1], NPoint)\n",
    "        yPlot = self.eval(xPlot)\n",
    "        plt.plot(xPlot,yPlot,c='black',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvZbimS27NxM"
   },
   "outputs": [],
   "source": [
    "class GPRegressionMM1D:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.FullModel = None       # Full model\n",
    "        self.Centers = None         # list of coordinates of data points (\"x\")\n",
    "        self.Vals = None            # list of function values at data points (\"y\")\n",
    "        self.NbRBFs = 10            # Number of data points\n",
    "        self.TypeQuadrature = 'MC'  # Type of automatised sempling 'MC' (random) or 'Uniform' (deterministic)\n",
    "        self.l = 1.0                # length scale of exponential covariance Kernel\n",
    "        self.Covariance = None      # data covariance matrix\n",
    "        self.A = 1.0                # amplitude of exponential covariance Kernel\n",
    "        self.sigma = 1.e-8          # amplitude of white noise Kernel\n",
    "    \n",
    "    def BuildQuadrature(self):\n",
    "        if self.TypeQuadrature == 'MC':\n",
    "            self.Centers = 2.0 * np.random.rand(self.NbRBFs,1) - 1.0\n",
    "        elif self.TypeQuadrature == 'Uniform':\n",
    "            self.Centers = np.linspace(-1,1,self.NbRBFs)\n",
    "        else:\n",
    "            print('nod coded')\n",
    "    \n",
    "    def Fit(self):\n",
    "        self.Vals = np.zeros((self.NbRBFs,1))\n",
    "        for i in range(self.NbRBFs):\n",
    "            self.Vals[i] = self.FullModel.eval(self.Centers[i])\n",
    "        self.Covariance = np.zeros((len(self.Centers),len(self.Centers)))\n",
    "\n",
    "        #self.Covariance = self.sigma * np.identity(len(self.Centers))           # Covariance matrix is to be updated\n",
    "        for i in range(len(self.Centers)):\n",
    "            for j in range(len(self.Centers)):\n",
    "                distance = self.Centers[i] - self.Centers[j]\n",
    "                self.Covariance[i,j] = self.A * np.exp( -1.0/(2*self.l**2) * distance**2 )\n",
    "        \n",
    "        self.Covariance = self.Covariance + self.sigma * np.identity(len(self.Centers))\n",
    "\n",
    "        #print('np.linalg.cond(self.Covariance) ', np.linalg.cond(self.Covariance))\n",
    "    \n",
    "    def eval(self,x) :\n",
    "        CrossCo = np.zeros((len(self.Centers),1))\n",
    "        for i in range(len(self.Centers)):\n",
    "            #CrossCo[i] = 0.                                                     # Covariance vector is to be updated\n",
    "            distance = self.Centers[i] - x\n",
    "            CrossCo[i] = self.A * np.exp( -1.0/(2*self.l**2) * distance**2 )\n",
    "        Tmp = np.linalg.solve(self.Covariance,self.Vals)\n",
    "        f = np.dot(np.transpose(CrossCo),Tmp)\n",
    "        \n",
    "        Tmp = np.linalg.solve(self.Covariance,CrossCo)\n",
    "        Covf = self.A - np.dot(np.transpose(CrossCo),Tmp)\n",
    "        return f, Covf\n",
    "\n",
    "    def LogLikelihood(self) :\n",
    "        Tmp = np.linalg.solve(self.Covariance,self.Vals)\n",
    "        # https://blogs.sas.com/content/iml/2012/10/31/compute-the-log-determinant-of-a-matrix.html\n",
    "        LogLike = - 0.5 * np.dot(np.transpose(self.Vals),Tmp) - 0.5 * self.Covariance.shape[0] * np.log(2*np.pi) #(eq. 2.30 rasmussen book Gaussian Process for Machine Learning)\n",
    "        #logdet = np.log(np.linalg.det(self.Covariance))\n",
    "        sign, logdet = np.linalg.slogdet(self.Covariance)\n",
    "        LogLike = LogLike - 0.5*logdet\n",
    "        return LogLike\n",
    "\n",
    "    def Objectif(self,mu):\n",
    "        self.l = np.exp(mu[0]) \n",
    "        self.A = np.exp(mu[1]) \n",
    "        self.sigma = np.exp(mu[2])\n",
    "        self.Fit() # recompute data covariance\n",
    "        return -1.0*self.LogLikelihood()\n",
    "\n",
    "    def Optimise(self):\n",
    "        muInit = [0.1 ,  0., -10]\n",
    "        InitVal = self.Objectif(muInit)\n",
    "        #mu_opt = fmin_bfgs( self.Objectif, muInit , gtol=1e-3)\n",
    "        mu_opt = fmin( self.Objectif , muInit )  \n",
    "        print('optimal parameter:', mu_opt, ' f value at optimal :', self.Objectif(mu_opt), 'Init : ',  muInit ,  ' f value at init :', InitVal )\n",
    "\n",
    "\n",
    "    def plot(self,xLim,NPoint):\n",
    "        xPlot = np.linspace(xLim[0],xLim[1], NPoint)\n",
    "        # plot posterior mean and 95% credible region\n",
    "        yPlot = np.copy(xPlot)\n",
    "        yPlotP = np.copy(xPlot)\n",
    "        yPlotM= np.copy(xPlot)\n",
    "        for i in range(len(xPlot)):\n",
    "            f, Covf = self.eval(xPlot[i])\n",
    "            yPlot[i] = f\n",
    "            yPlotP[i] = f + 1.96 * Covf\n",
    "            yPlotM[i] = f - 1.96 * Covf\n",
    "        plt.plot(xPlot,yPlot,'blue')\n",
    "        plt.plot(xPlot,yPlotP,'r')\n",
    "        plt.plot(xPlot,yPlotM,'g')\n",
    "        plt.scatter(self.Centers,self.Vals, marker='o', c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "98jGgDDjFNsw",
    "outputId": "a0913013-fd0a-42d6-d9cd-31126e6b8ea7"
   },
   "outputs": [],
   "source": [
    "def GaussianProcessMetaModel1D():\n",
    "\n",
    "    print(' ----------------------------------')\n",
    "    print(' ---------- Exercise GP -----------')\n",
    "    print(' ----------------------------------')\n",
    "    print(' 1. Implement an exponential Kernel: modify the Covariance matrix and the cross-covariance vector')\n",
    "    print(' 2. Propose a Greedy algorithm to iteratively add points to the data set')\n",
    "    print(' 3. Implement the automatic choice of the covariance length scale by maximising the data likelihood')\n",
    "\n",
    "    NSampling = 4\n",
    "    SmoothingLength = 0.2\n",
    "\n",
    "    #TypeTypeQuadrature = 'Uniform'\n",
    "    TypeTypeQuadrature = 'MC'\n",
    "\n",
    "    RidgeCoeff = 1e-10\n",
    "    np.random.seed(11)\n",
    "\n",
    "    print(' ---------------------------------------')\n",
    "    print(' ---------- Gaussian process -----------')\n",
    "    print(' ---------------------------------------')\n",
    "\n",
    "    plt.figure()\n",
    "    M = SimpleModel1D()\n",
    "    #MM = KernelRegressionMM1D() \n",
    "    MM = GPRegressionMM1D() \n",
    "    MM.FullModel = M\n",
    "    MM.TypeQuadrature = TypeTypeQuadrature\n",
    "    MM.NbRBFs = NSampling # Number of uniformly, randomly distributed radial basis functions\n",
    "    MM.l = SmoothingLength # length scale of Kernel smoother\n",
    "    MM.RidgeCoeff = RidgeCoeff\n",
    "    MM.BuildQuadrature()\n",
    "    MM.Fit()\n",
    "    MM.plot([-1,1],100)\n",
    "    M.plot([-1,1],100)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(' ------------------------------------------------------')\n",
    "    print(' ---------- Optimisation of data likelihood -----------')\n",
    "    print(' ------------------------------------------------------')\n",
    "\n",
    "    print('log likelihood',MM.LogLikelihood())\n",
    "    MM.Optimise()\n",
    "    MM.plot([-1,1],100)\n",
    "    M.plot([-1,1],100)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "GaussianProcessMetaModel1D() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE8QF9thN3U1"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Exercise 4: Bayesian optimisation**\n",
    "\n",
    "Suggest and implement a Greedy algorithm to iteratively add points to the data set based on (a) minimising uncertainty (b) finding the location of the minimum of the function\n",
    "\n",
    "Note: look for comment \"change this\" in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u1ururROGZ-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "class SimpleModel1D:\n",
    "    \n",
    "    def eval(self,x) :\n",
    "        f = np.sin(3.0*x-2.25) + abs(x) + x\n",
    "        return f\n",
    "    \n",
    "    def plot(self,xLim,NPoint):\n",
    "        xPlot = np.linspace(xLim[0],xLim[1], NPoint)\n",
    "        yPlot = self.eval(xPlot)\n",
    "        plt.plot(xPlot,yPlot,c='black',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95BW6FqiyVa9"
   },
   "outputs": [],
   "source": [
    "class GPRegressionMM1D:\n",
    "    \n",
    "    def __init__(self):\n",
    "         \n",
    "        self.FullModel = None       # Full model\n",
    "        self.Centers = None         # list of coordinates of data points (\"x\")\n",
    "        self.Vals = None            # list of function values at data points (\"y\")\n",
    "        self.NbRBFs = 10            # Number of data points\n",
    "        self.TypeQuadrature = 'MC'  # Type of automatised sempling 'MC' (random) or 'Uniform' (deterministic)\n",
    "        self.l = 1.0                # length scale of exponential covariance Kernel\n",
    "        self.Covariance = None      # data covariance matrix\n",
    "        self.A = 1.0                # amplitude of exponential covariance Kernel\n",
    "        self.sigma = 1.e-8          # amplitude of white noise Kernel\n",
    "        self.ParameterSpace = [-1,1]\n",
    "    \n",
    "    def BuildQuadrature(self):\n",
    "        if self.TypeQuadrature == 'MC':\n",
    "            self.Centers = (self.ParameterSpace[1] - self.ParameterSpace[0]) * np.random.rand(self.NbRBFs,1) + self.ParameterSpace[0]\n",
    "        elif self.TypeQuadrature == 'Uniform':\n",
    "            self.Centers = np.linspace(-1,1,self.NbRBFs)\n",
    "        else:\n",
    "            print('nod coded')\n",
    "    \n",
    "    def Fit(self):\n",
    "        self.Vals = np.zeros((len(self.Centers),1))\n",
    "        for i in range(len(self.Centers)):\n",
    "            self.Vals[i] = self.FullModel.eval(self.Centers[i])\n",
    "        self.Covariance = np.zeros((len(self.Centers),len(self.Centers)))\n",
    "        for i in range(len(self.Centers)):\n",
    "            for j in range(len(self.Centers)):\n",
    "                distance = self.Centers[i] - self.Centers[j]\n",
    "                self.Covariance[i,j] = self.A * np.exp( -1.0/(2*self.l**2) * distance**2 )\n",
    "\n",
    "        self.Covariance = self.Covariance + self.sigma * np.identity(len(self.Centers))      \n",
    "        print('np.linalg.cond(self.Covariance) ', np.linalg.cond(self.Covariance))\n",
    "    \n",
    "    def eval(self,x) :\n",
    "        CrossCo = np.zeros((len(self.Centers),1))\n",
    "        for i in range(len(self.Centers)):\n",
    "            distance = self.Centers[i] - x\n",
    "            CrossCo[i] = self.A * np.exp( -1.0/(2*self.l**2) * distance**2 )\n",
    "        Tmp = np.linalg.solve(self.Covariance,self.Vals)\n",
    "        f = np.dot(np.transpose(CrossCo),Tmp)\n",
    "        \n",
    "        Tmp = np.linalg.solve(self.Covariance,CrossCo)\n",
    "        Covf = self.A - np.dot(np.transpose(CrossCo),Tmp)\n",
    "        return f, Covf\n",
    "\n",
    "    def OptimNewPoint(self) :\n",
    "        #NewPoint = 2.0 * np.random.rand(1,1) - 1.0 # change this !!!\n",
    "        #grid_search = np.linspace(-1, 1, 100)\n",
    "        grid_search = 2.0*np.random.rand(50,1) - 1.0\n",
    "        min_val = 1.0e10\n",
    "        for i in range(len(grid_search)):\n",
    "          f, Covf = self.eval(grid_search[i])\n",
    "          if min_val > f - 1.96*Covf:\n",
    "            min_val = f - 1.96*Covf\n",
    "            NewPoint = grid_search[i]\n",
    "\n",
    "        print('NewPoint',NewPoint)\n",
    "        self.Centers = np.append(self.Centers,NewPoint)\n",
    "\n",
    "    def ActiveLearning(self,NActiveLearning) :\n",
    "        for i in range(NActiveLearning):\n",
    "            self.OptimNewPoint()\n",
    "            self.Fit()\n",
    "            self.plot(self.ParameterSpace,100)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "    \n",
    "    def plot(self,xLim,NPoint):\n",
    "        xPlot = np.linspace(xLim[0],xLim[1], NPoint)\n",
    "        # plot posterior mean and 95% credible region\n",
    "        yPlot = np.copy(xPlot)\n",
    "        yPlotP = np.copy(xPlot)\n",
    "        yPlotM= np.copy(xPlot)\n",
    "        for i in range(len(xPlot)):\n",
    "            f, Covf = self.eval(xPlot[i])\n",
    "            yPlot[i] = f\n",
    "            yPlotP[i] = f + 1.96 * Covf\n",
    "            yPlotM[i] = f - 1.96 * Covf\n",
    "        plt.plot(xPlot,yPlot,'blue')\n",
    "        plt.plot(xPlot,yPlotP,'r')\n",
    "        plt.plot(xPlot,yPlotM,'g')\n",
    "        plt.scatter(self.Centers,self.Vals, marker='o', c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IIN9me5DZbJQ",
    "outputId": "87a97ce3-5ec4-4956-94b9-645e99750dfd"
   },
   "outputs": [],
   "source": [
    "def GaussianProcessMetaModel1D():\n",
    "\n",
    "    print(' ----------------------------------')\n",
    "    print(' ---------- Exercise GP -----------')\n",
    "    print(' ----------------------------------')\n",
    "    print(' 1. Implement an exponential Kernel Covariance matrix ')\n",
    "    print(' 2. Propose a Greedy algorithm to iteratively add points to the data set')\n",
    "    print(' 3. Implement the automatic choice of the covariance length scale by maximising the data likelihood')\n",
    "\n",
    "    NSampling = 4\n",
    "    SmoothingLength = 0.2\n",
    "\n",
    "    #TypeTypeQuadrature = 'Uniform'\n",
    "    TypeTypeQuadrature = 'MC'\n",
    "\n",
    "    RidgeCoeff = 1e-10\n",
    "    np.random.seed(11)\n",
    "\n",
    "    print(' ---------------------------------------')\n",
    "    print(' ---------- Gaussian process -----------')\n",
    "    print(' ---------------------------------------')\n",
    "\n",
    "    plt.figure()\n",
    "    M = SimpleModel1D()\n",
    "    #MM = KernelRegressionMM1D() \n",
    "    MM = GPRegressionMM1D() \n",
    "    MM.FullModel = M\n",
    "    MM.TypeQuadrature = TypeTypeQuadrature\n",
    "    MM.ParameterSpace = [-1,1]\n",
    "    MM.NbRBFs = NSampling # Number of uniformly, randomly distributed radial basis functions\n",
    "    MM.l = SmoothingLength # length scale of Kernel smoother\n",
    "    MM.RidgeCoeff = RidgeCoeff\n",
    "    MM.BuildQuadrature()\n",
    "    MM.Fit()\n",
    "    MM.plot(MM.ParameterSpace,100)\n",
    "    M.plot([-1,1],100)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(' --------------------------------------')\n",
    "    print(' ---------- Active Learning -----------')\n",
    "    print(' --------------------------------------')\n",
    "\n",
    "    NActiveLearning = 4\n",
    "    MM.ActiveLearning(NActiveLearning)\n",
    "\n",
    "GaussianProcessMetaModel1D() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf77Gz4CHDN_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MetaModelAllExercisesSolutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
