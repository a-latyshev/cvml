{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single layer Neural Network\n",
    "\n",
    "In this notebook, we will code a single neuron and use it as a linear classifier with two inputs. The tuning of the neuron parameters is done by backpropagation using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib to display the data\n",
    "import matplotlib\n",
    "matplotlib.rc('font', size=16)\n",
    "matplotlib.rc('xtick', labelsize=16) \n",
    "matplotlib.rc('ytick', labelsize=16) \n",
    "from matplotlib import pyplot as plt, cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's create some labeled data in the form of (X, y) with an associated class which can be 0 or 1. For this we can use the function `make_blobs` in the `sklearn.datasets` module. Here we use 2 centers with coordinates (-0.5, -1.0) and (1.0, 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=..., random_state=42, centers=[...])\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our training data using `plt.scatter` to have a first visualization. Here we color the points with their labels stored in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(..., ..., c=y.squeeze(), edgecolors='gray')\n",
    "plt.title('training data with labels')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Here we play with popular activation functions like heaviside (step function), tanh, ReLu or sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return ...\n",
    "\n",
    "def sigmoid(x):\n",
    "    return ...\n",
    "\n",
    "def ReLU(x):\n",
    "    return ...\n",
    "\n",
    "def leaky_ReLU(x, alpha=0.1):\n",
    "    return ...\n",
    "\n",
    "def tanh(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot with all our activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "plt.figure()\n",
    "x = np.arange(-pi, pi, 0.01)\n",
    "plt.axhline(y=0., color='gray', linestyle='dashed')\n",
    "plt.axhline(y=-1, color='gray', linestyle='dashed')\n",
    "plt.axhline(y=1., color='gray', linestyle='dashed')\n",
    "plt.axvline(x=0., color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xlim(-pi, pi)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "plt.title('activation functions', fontsize=16)\n",
    "plt.plot(x, ..., label='heavyside', linewidth=3)\n",
    "plt.plot(x, ..., label='sigmoid', linewidth=3)\n",
    "plt.plot(x, ..., label='tanh', linewidth=3)\n",
    "plt.plot(x, ..., label='ReLU', linewidth=3)\n",
    "plt.plot(x, ..., label='leaky ReLU', linewidth=3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients of the activation functions\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return ...\n",
    "\n",
    "def relu_grad(x):\n",
    "    return ...\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradients of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(-pi, pi, 0.01)\n",
    "plt.plot(x, sigmoid_grad(x), label='sigmoid gradient', linewidth=3)\n",
    "plt.plot(x, relu_grad(x), label='ReLU gradient', linewidth=3)\n",
    "plt.plot(x, tanh_grad(x), label='tanh gradient', linewidth=3)\n",
    "plt.xlim(-pi, pi)\n",
    "plt.title('activation function derivatives', fontsize=16)\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_linewidth(2)\n",
    "plt.savefig('activation_functions_derivatives.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN implementation\n",
    "\n",
    "A simple neuron with two inputs $(x_1, x_2)$ which applies an affine transform of weigths $(w_1, w_2)$ and bias $w_0$.\n",
    "\n",
    "The neuron compute the quantity called activation $a=\\sum_i w_i x_i + w_0 = w_0 + w_1 x_1 + w_2 x_2$\n",
    "\n",
    "This quantity is send to the activation function chosen to be a sigmoid function here: $f(a)=\\dfrac{1}{1+e^{-a}}$\n",
    "\n",
    "$f(a)$ is the output of the neuron bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick implementation\n",
    "\n",
    "First let's implement our network in a concise fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "X, y = make_blobs(n_samples= 100, n_features=2, random_state=42, centers=[[-0.5, -1], [1, 1]])\n",
    "# adjust the sizes of our arrays\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "np.random.seed(2)\n",
    "W = randn(...)\n",
    "print('* model params: {}'.format(W.tolist()))\n",
    "eta = 1e-2  # learning rate\n",
    "n_epochs = 50\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # forward pass\n",
    "    y_pred = ...\n",
    "    loss = ...\n",
    "    print(t, loss)\n",
    "\n",
    "    # backprop\n",
    "    grad_y_pred = ...\n",
    "    grad_W = ...\n",
    "\n",
    "    # update rule\n",
    "    W -= ...\n",
    "print('* new model params: {}'.format(W.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular implementation\n",
    "\n",
    "Now let's create a class to represent our neural network to have more flexibility and modularity. This will prove to be useful later when we add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNeuralNetwork:\n",
    "    \"\"\"A simple artificial neuron with a single layer and two inputs. \n",
    "    \n",
    "    This type of network is called a Single Layer Neural Network and belongs to \n",
    "    the Feed-Forward Neural Networks. Here, the activation function is a sigmoid, \n",
    "    the loss is computed using the squared error between the target and \n",
    "    the prediction. Learning the parameters is achieved using back-propagation \n",
    "    and gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, rand_seed=42):\n",
    "        \"\"\"Initialisation routine.\"\"\"\n",
    "        np.random.seed(rand_seed)\n",
    "        self.W = ...  # weigths\n",
    "        self.eta = eta  # learning rate\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Our activation function.\"\"\"\n",
    "        return ...\n",
    "    \n",
    "    def sigmoid_grad(self, x):\n",
    "        \"\"\"Gradient of the sigmoid function.\"\"\"\n",
    "        return ...\n",
    "    \n",
    "    def predict(self, X, bias_trick=True):\n",
    "        X = np.atleast_2d(X)\n",
    "        if bias_trick:\n",
    "            # bias trick: add a column of 1 to X\n",
    "            X = np.c_[np.ones((X.shape[0])), X]\n",
    "        return ...\n",
    "    \n",
    "    def loss(self, X, y, bias_trick=False):\n",
    "        \"\"\"Compute the squared error loss for a given set of inputs.\"\"\"\n",
    "        y_pred = self.predict(X, bias_trick=bias_trick)\n",
    "        y_pred = y_pred.reshape((y_pred.shape[0], 1))\n",
    "        loss = ...\n",
    "        return loss\n",
    "        \n",
    "    def back_propagation(self, X, y):\n",
    "        \"\"\"Conduct backpropagation to update the weights.\"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "        y_pred = self.sigmoid(...).reshape((X.shape[0], 1))\n",
    "        grad_y_pred = ...\n",
    "        grad_W = ...\n",
    "\n",
    "        # update weights\n",
    "        self.W -= ...\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=10, method='batch', save_fig=False):\n",
    "        \"\"\"Perform gradient descent on a given number of epochs to update the weights.\"\"\"\n",
    "        # bias trick: add a column of 1 to X\n",
    "        X = np.c_[np.ones((X.shape[0])), X]\n",
    "        self.loss_history.append(self.loss(X, y))  # initial loss\n",
    "        for i_epoch in range(n_epochs):\n",
    "            if method == 'batch':\n",
    "                # perform backprop on the whole training set (batch)\n",
    "                self.back_propagation(X, y)\n",
    "                # weights were updated, compute the loss\n",
    "                loss = self.loss(X, y)\n",
    "                self.loss_history.append(loss)\n",
    "                print(i_epoch, self.loss_history[-1])\n",
    "            else:\n",
    "                # here we update the weight for every data point (SGD)\n",
    "                for (xi, yi) in zip(X, y):\n",
    "                    self.back_propagation(xi, yi)\n",
    "                    # weights were updated, compute the loss\n",
    "                    loss = self.loss(X, y)\n",
    "                    self.loss_history.append(loss)\n",
    "            if save_fig:\n",
    "                self.plot_model(i_epoch, save=True, display=False)\n",
    "\n",
    "    def decision_boundary(self, x):\n",
    "        \"\"\"Return the decision boundary in 2D.\"\"\"\n",
    "        return ...\n",
    "    \n",
    "    def plot_model(self, i_epoch=-1, save=False, display=True):\n",
    "        \"\"\"Build a figure to vizualise how the model perform.\"\"\"\n",
    "        xx0, xx1 = np.arange(-3, 3.1, 0.1), np.arange(-3, 4.1, 0.1)\n",
    "        XX0, XX1 = np.meshgrid(xx0, xx1)\n",
    "        # apply the model to the grid\n",
    "        y_an = np.empty(len(XX0.ravel()))\n",
    "        i = 0\n",
    "        for (x0, x1) in zip(XX0.ravel(), XX1.ravel()):\n",
    "            y_an[i] = self.predict(np.array([x0, x1]))\n",
    "            i += 1\n",
    "        y_an = y_an.reshape((len(xx1), len(xx0)))\n",
    "        figure = plt.figure(figsize=(12, 4))\n",
    "        ax1 = plt.subplot(1, 3, 1)\n",
    "        #ax1.set_title(r'$w_0=%.3f$, $w_1=%.3f$, $w_2=%.3f$' % (self.W[0], self.W[1], self.W[2]))\n",
    "        ax1.set_title(\"current prediction\")\n",
    "        ax1.contourf(XX0, XX1, y_an, alpha=.5)\n",
    "        ax1.scatter(X[:, 0], X[:, 1], c=y.squeeze(), edgecolors='gray')\n",
    "        ax1.set_xlim(-3, 3)\n",
    "        ax1.set_ylim(-3, 4)\n",
    "        print(ax1.get_xlim())\n",
    "        x = np.array(ax1.get_xlim())\n",
    "        ax1.plot(x, self.decision_boundary(x), 'k-', linewidth=2)\n",
    "        ax2 = plt.subplot(1, 3, 2)\n",
    "        x = np.arange(3)  # the label locations\n",
    "        rects1 = ax2.bar(x, [self.W[0, 0], self.W[1, 0], self.W[2, 0]])\n",
    "        ax2.set_title('model parameters')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([r'$w_0$', r'$w_1$', r'$w_2$'])\n",
    "        ax2.set_ylim(-1, 2)\n",
    "        ax2.set_yticks([0, 2])\n",
    "        ax2.axhline(xmin=0, xmax=2)\n",
    "        ax3 = plt.subplot(1, 3, 3)\n",
    "        ax3.plot(self.loss_history, c='lightgray', lw=2)\n",
    "        if i_epoch < 0:\n",
    "            i_epoch = len(self.loss_history) - 1\n",
    "        ax3.plot(i_epoch, self.loss_history[i_epoch], 'o')\n",
    "        ax3.set_title('loss evolution')\n",
    "        ax3.set_yticks([])\n",
    "        plt.subplots_adjust(left=0.05, right=0.98)\n",
    "        if save:\n",
    "            plt.savefig('an_%02d.png' % i_epoch)\n",
    "        if display:\n",
    "            plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model on the data set\n",
    "\n",
    "Create two blobs with $n=1000$ data points.\n",
    "\n",
    "Instantiate the model with $\\eta$=0.1 and a random seed of 2.\n",
    "\n",
    "Train the model using the batch gradient descent on 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=..., n_features=2, random_state=42, centers=[[-0.5, -1], [1, 1]])\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "an1 = SingleLayerNeuralNetwork(...)\n",
    "print('* init model params: {}'.format(an1.W.tolist()))\n",
    "print(an1.loss(X, y, bias_trick=True))\n",
    "an1.fit(X, y, n_epochs=..., method='batch', save_fig=False)\n",
    "print('* new model params: {}'.format(an1.W.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained our model, plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an1.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to train another network using SGD. Use only 1 epoch since with SGD, we are updating the weights with every training point (so $n$ times per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an2 = SingleLayerNeuralNetwork(eta=0.1, rand_seed=2)\n",
    "print('* init model params: {}'.format(an2.W.tolist()))\n",
    "an2.fit(X, y, n_epochs=1, method='SGD', save_fig=False)\n",
    "print('* new model params: {}'.format(an2.W.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the difference in terms of loss evolution using batch or stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(an1.loss_history[:], label='batch GD')\n",
    "plt.plot(an2.loss_history[0::100], label='stochastic GD')\n",
    "plt.ylim(0, 2000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an2.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Our single layer network using the logistic function for activation is very similar to the logistic regression we saw in a previous tutorial. We can easily compare our result with the logistic regression using `sklearn` toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, n_features=2, random_state=42, centers=[[-0.5, -1], [1, 1]])\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X, y)\n",
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(-3, 3.1, 62).reshape(-1, 1),\n",
    "        np.linspace(-3, 4.1, 72).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "contour = plt.contourf(x0, x1, zz, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='gray')\n",
    "\n",
    "# decision boundary\n",
    "x_bounds = np.array([-3, 3])\n",
    "boundary = -(log_reg.coef_[0][0] * x_bounds + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "plt.plot(x_bounds, boundary, \"k-\", linewidth=3)\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
